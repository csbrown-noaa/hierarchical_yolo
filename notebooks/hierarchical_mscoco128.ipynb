{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383e1ec7-d1e6-452a-a31b-03fb75e105f4",
   "metadata": {},
   "source": [
    "# Hierarchical COCO128 example\n",
    "\n",
    "This example explores training a hierarchical model on the [COCO128 dataset](https://docs.ultralytics.com/datasets/detect/coco128/), which is a very small subset of the\n",
    "classic [COCO dataset](https://cocodataset.org).  COCO128 is mostly intended as a \"sanity check\" dataset to quickly make sure that a model architecture and training routine are\n",
    "not utterly broken before embarking on a longer and more expensive training over a larger dataset.\n",
    "\n",
    "This example assumes some familiarity with the ultralytics family of YOLO models.  If you have never used these, you had ought to at least explore the [quickstart](https://docs.ultralytics.com/quickstart/) and the [ultralytics COCO128 example](https://docs.ultralytics.com/datasets/detect/coco128/).\n",
    "\n",
    "This example does not assume that you have any familiarity with convolutional neural networks or the mathematics of probability or MLOps or methodologies for hyperparameter selection.  However, these topics would be useful if you actually intend to use the hierarchical model outside of this toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216dae38-b662-4dd4-9552-f957b412ef8a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1b6d3d-a9cf-489b-aa23-9e4af40c834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "from hierarchical_yolo.mscoco_hierarchical_model import MSCOCOHierarchicalDetectionTrainer\n",
    "import hierarchical_yolo.yolo_utils\n",
    "import hierarchical_loss.path_utils\n",
    "import hierarchical_loss.viz_utils\n",
    "import ultralytics\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a534c9-bbdc-4637-9105-4cb6bd095ce3",
   "metadata": {},
   "source": [
    "### Data and Configs\n",
    "\n",
    "Ultralytics YOLO requires multiple configuration and weight files, and the default storage location for the trained models (runs) are not always so great.\n",
    "You should inspect these various paths to ensure that they align with your data management strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea05de7-8a8e-4908-9542-2cf4110ebc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics default settings.  See https://docs.ultralytics.com/quickstart/#ultralytics-settings\n",
    "# The root directory where datasets are expected to be stored, and where ultralytics will download curated datasets\n",
    "DATASETS = ultralytics.settings['datasets_dir']\n",
    "# The root directory for model weights.  In particular, where ultralytics will download pre-trained model .pt weight files\n",
    "WEIGHTS = ultralytics.settings['weights_dir']\n",
    "# The root directory where training runs will be stored, including evaluation artifacts and trained model .pt weight files\n",
    "RUNS = ultralytics.settings['runs_dir']\n",
    "\n",
    "# The pre-trained model to use.  Note the v8 (version 8) and the n (nano-scale)\n",
    "YOLO_BASE_WEIGHTS = os.path.join(WEIGHTS, \"yolov8n.pt\")\n",
    "\n",
    "# Configure project runs save locations.\n",
    "BASE_PROJECT = os.path.join(RUNS, 'coco128')\n",
    "HIERARCHICAL_PROJECT = os.path.join(RUNS, 'hierarchical_coco128')\n",
    "\n",
    "# Find where data is downloaded/stored.  This is where ultralytics will download the coco128 data.\n",
    "DATA = os.path.join(DATASETS, 'coco128')\n",
    "IMAGES_PATH = os.path.join(DATA, 'images', 'train2017')\n",
    "\n",
    "# Find model configurations.  These are in the repository https://github.com/csbrown-noaa/hierarchical_yolo.\n",
    "# If you aren't running this from the cloned repo, you will need to go acquire these and change MODEL_CONFIGS to reflect the location of the model config files.\n",
    "MODEL_CONFIGS = '../hierarchical_yolo/models'\n",
    "COCO128_MODELS = os.path.join(MODEL_CONFIGS, 'coco128')\n",
    "YOLO_MODEL_YAML = os.path.join(COCO128_MODELS, 'hierarchicalcoco128yolov8.yaml')\n",
    "YOLO_DATASET_YAML = os.path.join(COCO128_MODELS, 'hierarchicalcoco128.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7d150-9da4-4c68-ba25-0405bf447489",
   "metadata": {},
   "source": [
    "### Run on GPU or CPU\n",
    "\n",
    "If a GPU is available, we can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11515535-3725-4738-a8b3-c54ba8d3900c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devices = list(range(torch.cuda.device_count()))\n",
    "devices = devices[0] if devices else 'cpu'\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bca779-0aa0-48d0-9114-ab0d5826a50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dff831ac-01e7-4c06-a221-4a76b0082dcd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## \"Regular\" YOLO model\n",
    "\n",
    "If you don't already have the pre-trained weight file and the coco128 dataset, this will automatically download them, so you probably want to run this at least once.\n",
    "You can make this cell runnable by selecting it and pressing the \"y\" key.  \n",
    "\n",
    "This is also useful for comparison purposes.  You can compare predictions from this model with ones from the hierarchical model below.  However...\n",
    "\n",
    "***NB:*** The ultralytics YOLO models are pre-trained *on the COCO dataset*.  So, \"evaluating\" the performance of the model, even informally, on COCO or COCO-derived datasets is not sound.  As such, any comparisons should probably be limited to the format of predictions and how the hierarchical predictions are arranged vs. the \"flat\" predictions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f2dd491-0eb6-4f88-be69-3f8e45059ae6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Download and load a base model\n",
    "model = YOLO(YOLO_BASE_WEIGHTS)  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Download the data and train a base model\n",
    "base_coco_model = model.train(data=\"coco128.yaml\", epochs=1, imgsz=640, project=BASE_PROJECT, device=devices, val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b79e4-a563-4bfd-b71b-8abd27166b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02268c0a-3643-49af-8a07-604ab6fe9cd2",
   "metadata": {},
   "source": [
    "## Hierarchical YOLO model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89427b8-779e-4715-a6db-3fad6eab2970",
   "metadata": {},
   "source": [
    "### The Hierarchy\n",
    "\n",
    "The original COCO dataset does not include a hierarchical representation.  We have formulated one for this example.  It is displayed below.  Note that all of the leaf nodes are original COCO categories.\n",
    "\n",
    "***Technical Details for implementing your own hierarchy:*** If you want to train an ultralytics YOLO detection model on your own custom data, you will need to define the categories and the hierarchy.  \n",
    "\n",
    "For ultralytics YOLO, the categories are provided in the training data configuration yaml.  You can see this in action with [the original COCO128 yaml](https://github.com/csbrown-noaa/hierarchical_yolo/blob/main/hierarchical_yolo/models/coco128/coco128.yaml). The \"extended\" categories that include our custom \"higher-level\" categories are defined in the dataset yaml files that we define above, and which can be found [here](https://github.com/csbrown-noaa/hierarchical_yolo/blob/main/hierarchical_yolo/models/coco128/hierarchicalcoco128.yaml) and [here](https://github.com/csbrown-noaa/hierarchical_yolo/blob/main/hierarchical_yolo/models/coco128/hierarchicalcoco128yolov8.yaml).  Note the `nc` and `names` parameters in these files.  \n",
    "\n",
    "The hierarchy is defined as a `_hierarchy` property of [the `MSCOCOHierarchicalDetectionTrainer` class](https://github.com/csbrown-noaa/hierarchical_yolo/blob/main/hierarchical_yolo/mscoco_hierarchical_model.py).    However, these yaml files do not encode the actual hierarchical relationship between the categories.  The user must do this by extending the [`HierarchicalDetectionTrainer` class](https://github.com/csbrown-noaa/hierarchical_yolo/blob/main/hierarchical_yolo/hierarchical_detection.py) and providing a `_hierarchy` class property.  The [`MSCOCOHierarchicalDetectionTrainer` class](https://github.com/csbrown-noaa/hierarchical_yolo/blob/main/hierarchical_yolo/mscoco_hierarchical_model.py) provides a good example of how to accomplish this.  \n",
    "\n",
    "***Technical details for implementing your own custom loss:*** The `MSCOCOHierarchicalDetectionTrainer` class utilizes the custom loss and training classes defined in [the `hierarchical_yolo.hierarchical_detection` module](https://github.com/csbrown-noaa/hierarchical_yolo/blob/main/hierarchical_yolo/hierarchical_detection.py).  In order to extend this functionality to, say, segmentation, new loss and training classes would need to be defined.  Although these may look daunting, in reality, 99% of the code is copy-pasta from [the ultralytics implementations](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/loss.py#L194).  In particular, the \"new\" code is limited to propagating the `hierarchy` down the call stack to the loss function and replacing a few lines for the class loss to utilize the [`hierarchical_loss` library](https://github.com/csbrown-noaa/hierarchical_loss).  The existing bounding box loss and dfl loss remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3047a58-7384-4843-ae4b-2064a570e66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object : \n",
      "â”œâ”€â”€ accessory_and_item : \n",
      "â”‚   â”œâ”€â”€ home_and_office_item : \n",
      "â”‚   â”‚   â”œâ”€â”€ book : \n",
      "â”‚   â”‚   â”œâ”€â”€ clock : \n",
      "â”‚   â”‚   â”œâ”€â”€ scissors : \n",
      "â”‚   â”‚   â”œâ”€â”€ teddy bear : \n",
      "â”‚   â”‚   â””â”€â”€ vase : \n",
      "â”‚   â””â”€â”€ personal_accessory : \n",
      "â”‚       â”œâ”€â”€ backpack : \n",
      "â”‚       â”œâ”€â”€ handbag : \n",
      "â”‚       â”œâ”€â”€ suitcase : \n",
      "â”‚       â”œâ”€â”€ tie : \n",
      "â”‚       â””â”€â”€ umbrella : \n",
      "â”œâ”€â”€ food : \n",
      "â”‚   â”œâ”€â”€ dessert : \n",
      "â”‚   â”‚   â”œâ”€â”€ cake : \n",
      "â”‚   â”‚   â””â”€â”€ donut : \n",
      "â”‚   â”œâ”€â”€ fruit : \n",
      "â”‚   â”‚   â”œâ”€â”€ apple : \n",
      "â”‚   â”‚   â”œâ”€â”€ banana : \n",
      "â”‚   â”‚   â””â”€â”€ orange : \n",
      "â”‚   â”œâ”€â”€ prepared_meal : \n",
      "â”‚   â”‚   â”œâ”€â”€ hot dog : \n",
      "â”‚   â”‚   â”œâ”€â”€ pizza : \n",
      "â”‚   â”‚   â””â”€â”€ sandwich : \n",
      "â”‚   â””â”€â”€ vegetable : \n",
      "â”‚       â”œâ”€â”€ broccoli : \n",
      "â”‚       â””â”€â”€ carrot : \n",
      "â”œâ”€â”€ indoor_object : \n",
      "â”‚   â”œâ”€â”€ appliance_and_electronics : \n",
      "â”‚   â”‚   â”œâ”€â”€ appliance : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ microwave : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ oven : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ refrigerator : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ sink : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ toaster : \n",
      "â”‚   â”‚   â”‚   â””â”€â”€ toilet : \n",
      "â”‚   â”‚   â””â”€â”€ electronics : \n",
      "â”‚   â”‚       â”œâ”€â”€ cell phone : \n",
      "â”‚   â”‚       â”œâ”€â”€ keyboard : \n",
      "â”‚   â”‚       â”œâ”€â”€ laptop : \n",
      "â”‚   â”‚       â”œâ”€â”€ mouse : \n",
      "â”‚   â”‚       â”œâ”€â”€ remote : \n",
      "â”‚   â”‚       â””â”€â”€ tv : \n",
      "â”‚   â”œâ”€â”€ furniture : \n",
      "â”‚   â”‚   â”œâ”€â”€ bed : \n",
      "â”‚   â”‚   â”œâ”€â”€ chair : \n",
      "â”‚   â”‚   â”œâ”€â”€ couch : \n",
      "â”‚   â”‚   â”œâ”€â”€ dining table : \n",
      "â”‚   â”‚   â””â”€â”€ potted plant : \n",
      "â”‚   â”œâ”€â”€ kitchenware : \n",
      "â”‚   â”‚   â”œâ”€â”€ bottle : \n",
      "â”‚   â”‚   â”œâ”€â”€ bowl : \n",
      "â”‚   â”‚   â”œâ”€â”€ cup : \n",
      "â”‚   â”‚   â”œâ”€â”€ fork : \n",
      "â”‚   â”‚   â”œâ”€â”€ knife : \n",
      "â”‚   â”‚   â”œâ”€â”€ spoon : \n",
      "â”‚   â”‚   â””â”€â”€ wine glass : \n",
      "â”‚   â””â”€â”€ personal_care_item : \n",
      "â”‚       â”œâ”€â”€ hair dryer : \n",
      "â”‚       â””â”€â”€ toothbrush : \n",
      "â”œâ”€â”€ living_thing : \n",
      "â”‚   â”œâ”€â”€ animal : \n",
      "â”‚   â”‚   â”œâ”€â”€ domestic_animal : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ bird : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ cat : \n",
      "â”‚   â”‚   â”‚   â””â”€â”€ dog : \n",
      "â”‚   â”‚   â”œâ”€â”€ livestock_animal : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ cow : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ horse : \n",
      "â”‚   â”‚   â”‚   â””â”€â”€ sheep : \n",
      "â”‚   â”‚   â””â”€â”€ wild_animal : \n",
      "â”‚   â”‚       â”œâ”€â”€ bear : \n",
      "â”‚   â”‚       â”œâ”€â”€ elephant : \n",
      "â”‚   â”‚       â”œâ”€â”€ giraffe : \n",
      "â”‚   â”‚       â””â”€â”€ zebra : \n",
      "â”‚   â””â”€â”€ person : \n",
      "â”œâ”€â”€ outdoor_and_sports_object : \n",
      "â”‚   â”œâ”€â”€ sports_equipment : \n",
      "â”‚   â”‚   â”œâ”€â”€ ball_sport_equipment : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ baseball bat : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ baseball glove : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ sports ball : \n",
      "â”‚   â”‚   â”‚   â””â”€â”€ tennis racket : \n",
      "â”‚   â”‚   â”œâ”€â”€ board_sport_equipment : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ skateboard : \n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ snowboard : \n",
      "â”‚   â”‚   â”‚   â””â”€â”€ surfboard : \n",
      "â”‚   â”‚   â””â”€â”€ other_sports_equipment : \n",
      "â”‚   â”‚       â”œâ”€â”€ frisbee : \n",
      "â”‚   â”‚       â”œâ”€â”€ kite : \n",
      "â”‚   â”‚       â””â”€â”€ skis : \n",
      "â”‚   â””â”€â”€ street_fixture : \n",
      "â”‚       â”œâ”€â”€ bench : \n",
      "â”‚       â”œâ”€â”€ fire hydrant : \n",
      "â”‚       â”œâ”€â”€ parking meter : \n",
      "â”‚       â”œâ”€â”€ stop sign : \n",
      "â”‚       â””â”€â”€ traffic light : \n",
      "â””â”€â”€ vehicle : \n",
      "    â”œâ”€â”€ airplane : \n",
      "    â”œâ”€â”€ boat : \n",
      "    â””â”€â”€ land_vehicle : \n",
      "        â”œâ”€â”€ bicycle : \n",
      "        â”œâ”€â”€ bus : \n",
      "        â”œâ”€â”€ car : \n",
      "        â”œâ”€â”€ motorcycle : \n",
      "        â”œâ”€â”€ train : \n",
      "        â””â”€â”€ truck : \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hierarchical_loss.viz_utils.viz_tree(MSCOCOHierarchicalDetectionTrainer._hierarchy.raw_tree).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22fa052-261e-4105-844e-eb1395a2fa2f",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Note that loading the model is identical to the \"basic\" YOLO model.\n",
    "\n",
    "The differences to note are: \n",
    "\n",
    "1) utilizing the dataset yaml file (YOLO_MODEL_YAML) that includes the \"extended\" COCO categories that include our custom higher-level categories as seen above.  [See above](#The-Hierarchy) for more details.\n",
    "2) providing a custom `trainer` to the `train` method.  This custom trainer implements our hierarchy and the custom loss function that utilizes the hierarchy.  [See above](#The-Hierarchy) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b464048-b5c1-4d4b-b6c3-8e76a48c6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ no model scale passed. Assuming scale='n'.\n",
      "Transferred 319/355 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "hierarchical_model = YOLO(YOLO_MODEL_YAML).load(YOLO_BASE_WEIGHTS)  # build a new model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f28ae-516f-4c36-a3db-b97286f1a71f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.228 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.186 ðŸš€ Python-3.12.3 torch-2.7.0+cu126 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11165MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=20, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../hierarchical_yolo/models/coco128/hierarchicalcoco128.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=../hierarchical_yolo/models/coco128/hierarchicalcoco128yolov8.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=/home/noaa_brown/hierarchical_yolo2/weights/yolov8n.pt, profile=False, project=/home/noaa_brown/hierarchical_yolo2/runs/hierarchical_coco128, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/noaa_brown/hierarchical_yolo2/runs/hierarchical_coco128/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=False, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "WARNING âš ï¸ no model scale passed. Assuming scale='n'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1089331  ultralytics.nn.modules.head.Detect           [109, [64, 128, 256]]         \n",
      "hierarchicalcoco128YOLOv8 summary: 129 layers, 3,348,867 parameters, 3,348,851 gradients, 9.7 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1508.1Â±475.3 MB/s, size: 50.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/noaa_brown/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 128/128 183985.9it/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 953.6Â±788.1 MB/s, size: 52.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/noaa_brown/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 128/128 164836.0it/s 0.0s0s\n",
      "Plotting labels to /home/noaa_brown/hierarchical_yolo2/runs/hierarchical_coco128/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=8.8e-05, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/noaa_brown/hierarchical_yolo2/runs/hierarchical_coco128/train3\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100      2.97G      2.815       28.9      1.189        162        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.70it/s 11.5s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100      2.99G        2.7      31.11      1.161        198        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.73it/s 10.9s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/100      2.99G      2.609      31.53      1.143        226        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/100      2.99G      2.696       30.4      1.141        222        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/100      3.01G      2.749      30.78      1.155        269        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/100      3.01G      2.656      31.42      1.147        226        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.73it/s 11.0s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/100      3.01G      2.915      28.05      1.158        273        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/100      3.01G      2.952      27.87      1.143        204        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/100      3.01G      3.203      27.81       1.19        184        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/100      3.01G      2.917      27.17      1.178        173        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/100      3.01G      3.116      26.14      1.219        129        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/100      3.02G      3.086      26.97      1.208        166        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/100      3.02G       3.04      24.86      1.188        256        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/100      3.03G      3.105      25.02      1.163        208        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     15/100      3.05G      3.177      24.95      1.181        228        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     16/100      3.05G      3.138      23.49       1.22        212        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     17/100      3.05G      3.135      22.91       1.21        199        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     18/100      3.07G       3.09      22.96      1.191        182        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     19/100      3.09G      3.323       23.5      1.217        237        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     20/100      3.09G      3.232      22.06      1.215        161        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     21/100      3.09G      3.314      22.84      1.256        217        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     22/100      3.11G      3.291      21.67      1.218        196        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     23/100      3.13G      3.419      22.01      1.281        194        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     24/100      3.13G      3.352      20.71      1.201        256        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     25/100      3.13G      3.432       21.2       1.27        192        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     26/100      3.15G      3.425      21.28      1.258        142        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     27/100      3.15G      3.694      19.24      1.308        137        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     28/100      3.15G      3.436         19      1.229        284        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     29/100      3.15G      3.408      18.66      1.225        186        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     30/100      3.15G      3.532      20.08      1.266        230        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     31/100      3.15G      3.541      18.39      1.298        200        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     32/100      3.15G      3.492      17.95       1.26        186        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     33/100      3.15G      3.464      17.51      1.223        172        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     34/100      3.15G      3.739       17.7      1.306        278        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     35/100      3.15G      3.663      17.21      1.268        267        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     36/100      3.15G       3.55      18.18      1.255        162        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     37/100      3.15G      3.647      16.34       1.27        131        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     38/100      3.15G      3.774      15.76      1.282        211        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     39/100      3.15G      3.578      17.23      1.224        289        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     40/100      3.15G      3.703      15.81      1.299        247        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     41/100      3.15G      3.644      14.86      1.241        257        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     42/100      3.15G       3.65      15.22      1.246        333        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     43/100      3.15G      3.651      15.36      1.256        272        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.74it/s 10.8s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     44/100      3.15G      3.562      14.32      1.228        230        640:  75% â”â”â”â”â”â”â”â”â”â”€â”€â”€ 6/8 0.65it/s 9.4s"
     ]
    }
   ],
   "source": [
    "results = hierarchical_model.train(\n",
    "    data=YOLO_DATASET_YAML,\n",
    "    project=HIERARCHICAL_PROJECT,\n",
    "    epochs=100,\n",
    "    imgsz=640, \n",
    "    box=20, #upweight the box loss.  The marginal confidences tend to be lower for the hierarchical case, which makes the cls_loss higher, on average.\n",
    "    trainer=MSCOCOHierarchicalDetectionTrainer,\n",
    "    device=devices,\n",
    "    val=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975273ed-1a8d-43aa-a1ed-56dd3f40b419",
   "metadata": {},
   "source": [
    "### Model Results\n",
    "\n",
    "Now that we've trained a model, we can inspect the results.  Evaluating model performance with hierarchical categories does not fit with the usual paradigms for evaluating model performance, so we can't naively use [the existing ultralytics model evaluation metrics](https://docs.ultralytics.com/reference/utils/metrics/).  There exist various mechanisms for evaluating performance of hierarchical models, [e.g. here](https://www.kaggle.com/competitions/fathomnet-2025).  Also possible are casting the hierarchical problem into a \"flat\" problem by only considering the leaf nodes or the root nodes and comparing with a bespoke model trained on only those categories.  However, we do not perform any of this here, and limit ourselves to visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d37aa-f7ab-409e-a160-621189ea9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest model from the last training run for this project\n",
    "trained_models = os.listdir(HIERARCHICAL_PROJECT)\n",
    "model_numbers = map(lambda x: int(x[len('train'):]) if len(x) > len('train') else 0, trained_models)\n",
    "latest_model = 'train' + str(max(model_numbers))\n",
    "latest_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d870a9b-0971-4e45-8ff5-5d23e6fc2df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load up the weights from the last training run in evaluation mode\n",
    "trained_model = (\n",
    "    YOLO(os.path.join(HIERARCHICAL_PROJECT, latest_model, 'weights', 'best.pt')).to(devices)\n",
    ")\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b0eca-d7d2-4272-9442-0640f1e52f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in a test image from the training dataset.\n",
    "image_path = os.path.join(IMAGES_PATH, '000000000257.jpg')\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf16faa-6708-42db-a75c-ba43ca2238e9",
   "metadata": {},
   "source": [
    "#### Raw Predictions\n",
    "\n",
    "Probably, you won't need these.  However, we include them here to illustrate how we interpret the hierarchical predictions. \n",
    "\n",
    "Note the shape of these.  The first value in the shape, the number of rows, is 4 + number_of_categories.  The first 4 indices are the bounding box corners, and the remaining 109 are the class confidences.  There are 109 classes, including the 80 original COCO categories, and 29 additional \"higher-level\" categories, all of which you can see in the class tree above.  The second value, the number of columns, is the number of bounding box predictions made.  This is a fixed value based on our model architecture.  Typically, we filter these heavily due to large amounts of overlap, etc, to get only a handful of predictions using a method called [non maximum supression](https://medium.com/analytics-vidhya/non-max-suppression-nms-6623e6572536).  We will investigate this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3bf21-c0dc-4f88-9a7a-3cc61e62c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = hierarchical_yolo.yolo_utils.yolo_raw_predict(trained_model, [image], (640, 640))[0]\n",
    "raw_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005cd118-8a8b-4917-81c1-346a37a13ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw predictions for a single bounding box proposal\n",
    "ROOT_NODE_IDX = int(MSCOCOHierarchicalDetectionTrainer._hierarchy.roots)\n",
    "N_BBOX_INDICES = 4\n",
    "# This gives the index of the object with the highest confidence for the root \"object\" class\n",
    "one_full_pred_loc = int(torch.argmax(raw_predictions[N_BBOX_INDICES + ROOT_NODE_IDX]))\n",
    "one_full_obj_preds = raw_predictions[N_BBOX_INDICES:, one_full_pred_loc]\n",
    "one_full_obj_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc76e6-5d3f-4b8e-9196-90695406ed0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hierarchical_loss.viz_utils.viz_tree(\n",
    "    MSCOCOHierarchicalDetectionTrainer._hierarchy.index_tree, \n",
    "    name_map = MSCOCOHierarchicalDetectionTrainer._hierarchy.idx_to_node, \n",
    "    vals = list(map(lambda x: '{:.4f}'.format(x), one_full_obj_preds))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e153b-2382-4f21-a40a-618434960c58",
   "metadata": {},
   "source": [
    "#### Interpreting Hierarchical Predictions\n",
    "\n",
    "The way that the model is trained computes a class loss that interprets the raw predictions as *conditional probabilities*.  This means that the raw score corresponding to, say, \"train\" is not the model's confidence that the object is a train.  It is the model's confidence that the object is a train **GIVEN** that the object is a \"land_vehicle\".  The raw score corresponding to \"land_vehicle\", in turn, is conditioned on \"vehicle\", which is conditioned on \"object\".  So, if you look through the leaves of the scores above, you may notice very high confidences reported for nodes whose parentage actually have low scores.  These are actually interpreted by the model to be \"unlikely\", since the score is only conditional.  One can obtain the marginal confidences by multiplying down a branch:\n",
    "\n",
    "If $C_0, C_1, C_2, \\ldots, C_n$ are a branch of nodes in a hierarchy such that belonging to a leaf class, $C_n$ necessarily implies belonging to the parent class $C_{n-1}$, and so on up to the root node $C_0$, then the marginal confidence can be recovered as:\n",
    "\n",
    "$$ P(C_n) = P(C_n | C_{n-1}) * P(C_{n-1} | C_{n-2}) * \\ldots * P(C_1 | C_0) * P(C_0) $$\n",
    "\n",
    "The confidence for root nodes is not conditional, since there is nothing left to condition on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523fcec-31af-429e-b2de-bdbee6b53a64",
   "metadata": {},
   "source": [
    "#### Filtered Predictions\n",
    "\n",
    "The default filtering methods provided [as methods on the ultralytics YOLO model classes](https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode) perform filtering as thought the confidences are *marginal*.  Since we are interpreting our confidences as *conditional*, then these methods do not work correctly for our hierarchical output.  Therefore, we provide several convenience methods for interpreting the raw output.\n",
    "\n",
    "We employ the [non maximum supression provided by ultralytics](https://docs.ultralytics.com/reference/utils/nms/) on only the root categories to filter the results.  Furthermore, we filter the raw confidences across all categories to only choose an \"optimal path\", which essentially chases down the maximum marginal confidence at each layer in the hierarchy, beginning with the root.  Note that the first value in the first prediction is the index of the root node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf941ce-e42b-4476-8779-6aedd094a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_boxes, predicted_paths, predicted_path_scores = hierarchical_yolo.yolo_utils.hierarchical_predict(trained_model, MSCOCOHierarchicalDetectionTrainer._hierarchy.index_tree, [image])\n",
    "predicted_paths[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b251f22-13c6-443c-b9d4-62d9d438c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(MSCOCOHierarchicalDetectionTrainer._hierarchy.roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b1647-b108-4126-bdbe-3eae6ab6ee6a",
   "metadata": {},
   "source": [
    "#### More Filtering\n",
    "\n",
    "The `hierarchical_predict` method follows the prediction all the way from the root to a leaf.  We can further filter this by truncating the path once we reach a conditional confidence that is below a certain threshold.  Depending on the threshold chosen, the truncation results may not even include the root node, so we further filter these empty paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5a04a-ff1b-432f-a305-1a6221b6bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpaths, tscores = hierarchical_loss.path_utils.truncate_paths_marginals(predicted_paths[0], predicted_path_scores[0], threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac907e-c87a-4bbd-a551-6b0fdb9fcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fboxes, fpaths, fscores = hierarchical_loss.path_utils.filter_empty_paths(predicted_boxes[0], tpaths, tscores)\n",
    "fboxes.shape, len(fpaths), len(fscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9a59f-b6fc-4574-b309-2afa58220a4e",
   "metadata": {},
   "source": [
    "#### Visualizing the results\n",
    "\n",
    "Here, we draw the filtered predictions on the image, along with the furthest-down-the-hierarchy predicted category and the marginal confidence of that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc62fa8-2516-4740-a248-5c40dfdcd0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_confidences = [float(torch.prod(fscore)) for fscore in fscores]\n",
    "lowest_category_in_path = [MSCOCOHierarchicalDetectionTrainer._hierarchy.idx_to_node[fpath[-1]] for fpath in fpaths]\n",
    "hierarchical_loss.viz_utils.draw_boxes_on_image(image, fboxes.T, \n",
    "                    labels=lowest_category_in_path, \n",
    "                    scores=marginal_confidences, \n",
    "                    box_color=(0, 255, 0), text_color=(255, 255, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3118d3-a039-4ecc-9e60-41c7f15429bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a463fea-f9f8-4f7e-9925-d6324eae69f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4764d1c-6239-46dd-b3c9-bc15e1794ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e19018-7727-4937-9746-2f7af15a2794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadf070-75b7-4c88-b4fe-80fc487e03ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191cc3f-549e-4081-9e03-727b330f284d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec08b0-080c-41b5-861b-35dafe1efb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45fab4-d9e1-42aa-8cfd-0a0649640444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191621f-d0f6-4467-bc78-949f9703a4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
