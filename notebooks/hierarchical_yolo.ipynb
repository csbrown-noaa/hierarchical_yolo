{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764c8187-cbd4-4876-8265-8b9078e7fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "import pycocotools.coco\n",
    "import pycocowriter.coco2yolo\n",
    "import requests\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aec1b9-f9d8-444e-872d-e4325536c3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2deb8380-18f8-4e28-884a-90fb0aa3e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class v8DetectionHierarchicalLoss:\n",
    "    \"\"\"Criterion class for computing training losses for YOLOv8 object detection.\"\"\"\n",
    "\n",
    "    def __init__(self, model, tal_topk=10, hierarchy={}):  # model must be de-paralleled\n",
    "        # hiearchy should be {child_id: parent_id}\n",
    "        from ultralytics.utils.loss import TaskAlignedAssigner, BboxLoss\n",
    "        import torch.nn as nn\n",
    "        self.hierarchy = hierarchy\n",
    "        \"\"\"Initialize v8DetectionLoss with model parameters and task-aligned assignment settings.\"\"\"\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "        h = model.args  # hyperparameters\n",
    "\n",
    "        m = model.model[-1]  # Detect() module\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.hyp = h\n",
    "        self.stride = m.stride  # model strides\n",
    "        self.nc = m.nc  # number of classes\n",
    "        self.no = m.nc + m.reg_max * 4\n",
    "        self.reg_max = m.reg_max\n",
    "        self.device = device\n",
    "\n",
    "        self.use_dfl = m.reg_max > 1\n",
    "\n",
    "        self.assigner = TaskAlignedAssigner(topk=tal_topk, num_classes=self.nc, alpha=0.5, beta=6.0)\n",
    "        self.bbox_loss = BboxLoss(m.reg_max).to(device)\n",
    "        self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)\n",
    "\n",
    "    def preprocess(self, targets, batch_size, scale_tensor):\n",
    "        raise Exception(\"whatever!\")\n",
    "        \"\"\"Preprocess targets by converting to tensor format and scaling coordinates.\"\"\"\n",
    "        nl, ne = targets.shape\n",
    "        if nl == 0:\n",
    "            out = torch.zeros(batch_size, 0, ne - 1, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]  # image index\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            counts = counts.to(dtype=torch.int32)\n",
    "            out = torch.zeros(batch_size, counts.max(), ne - 1, device=self.device)\n",
    "            for j in range(batch_size):\n",
    "                matches = i == j\n",
    "                if n := matches.sum():\n",
    "                    out[j, :n] = targets[matches, 1:]\n",
    "            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\n",
    "        return out\n",
    "\n",
    "    def bbox_decode(self, anchor_points, pred_dist):\n",
    "        raise Exception(\"whatever!\")\n",
    "        \"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\n",
    "        if self.use_dfl:\n",
    "            b, a, c = pred_dist.shape  # batch, anchors, channels\n",
    "            pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "            # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "            # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\n",
    "        return dist2bbox(pred_dist, anchor_points, xywh=False)\n",
    "\n",
    "    def __call__(self, preds, batch):\n",
    "        \"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\"\"\"\n",
    "        ultralytics.utils.LOGGER.info(\"pickles!\")\n",
    "        loss = torch.zeros(3, device=self.device)  # box, cls, dfl\n",
    "        feats = preds[1] if isinstance(preds, tuple) else preds\n",
    "        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n",
    "            (self.reg_max * 4, self.nc), 1\n",
    "        )\n",
    "\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dtype = pred_scores.dtype\n",
    "        batch_size = pred_scores.shape[0]\n",
    "        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n",
    "        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n",
    "\n",
    "        # Targets\n",
    "        targets = torch.cat((batch[\"batch_idx\"].view(-1, 1), batch[\"cls\"].view(-1, 1), batch[\"bboxes\"]), 1)\n",
    "        targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\n",
    "        gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0.0)\n",
    "\n",
    "        # Pboxes\n",
    "        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n",
    "        # dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\n",
    "        # dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\n",
    "\n",
    "        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n",
    "            # pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\n",
    "            pred_scores.detach().sigmoid(),\n",
    "            (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n",
    "            anchor_points * stride_tensor,\n",
    "            gt_labels,\n",
    "            gt_bboxes,\n",
    "            mask_gt,\n",
    "        )\n",
    "\n",
    "        raise Exception(\"whatever!\")\n",
    "        mlflow.set_tag(\"debug_hook_called\", \"yes\")\n",
    "        mlflow.log_text(\"target shapes\", artifact_file=\"layer_output.log\")\n",
    "        mlflow.log_text(target_bboxes.shape, artifact_file=\"layer_output.log\")\n",
    "        mlflow.log_text(target_scores.shape, artifact_file=\"layer_output.log\")\n",
    "        target_scores_sum = max(target_scores.sum(), 1)\n",
    "\n",
    "        # Cls loss\n",
    "        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way        \n",
    "        \n",
    "        loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n",
    "\n",
    "        # Bbox loss\n",
    "        if fg_mask.sum():\n",
    "            target_bboxes /= stride_tensor\n",
    "            loss[0], loss[2] = self.bbox_loss(\n",
    "                pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask\n",
    "            )\n",
    "\n",
    "        loss[0] *= self.hyp.box  # box gain\n",
    "        loss[1] *= self.hyp.cls  # cls gain\n",
    "        loss[2] *= self.hyp.dfl  # dfl gain\n",
    "\n",
    "        return loss * batch_size, loss.detach()  # loss(box, cls, dfl)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96ff895c-527e-4363-8f67-e4c014c0f860",
   "metadata": {},
   "source": [
    "class v8DetectionLoss:\n",
    "    \"\"\"Criterion class for computing training losses for YOLOv8 object detection.\"\"\"\n",
    "\n",
    "    def __init__(self, model, tal_topk=10):  # model must be de-paralleled\n",
    "        from ultralytics.utils.loss import TaskAlignedAssigner, BboxLoss\n",
    "        import torch.nn as nn\n",
    "        \"\"\"Initialize v8DetectionLoss with model parameters and task-aligned assignment settings.\"\"\"\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "        h = model.args  # hyperparameters\n",
    "\n",
    "        m = model.model[-1]  # Detect() module\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.hyp = h\n",
    "        self.stride = m.stride  # model strides\n",
    "        self.nc = m.nc  # number of classes\n",
    "        self.no = m.nc + m.reg_max * 4\n",
    "        self.reg_max = m.reg_max\n",
    "        self.device = device\n",
    "\n",
    "        self.use_dfl = m.reg_max > 1\n",
    "\n",
    "        self.assigner = TaskAlignedAssigner(topk=tal_topk, num_classes=self.nc, alpha=0.5, beta=6.0)\n",
    "        self.bbox_loss = BboxLoss(m.reg_max).to(device)\n",
    "        self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)\n",
    "\n",
    "    def preprocess(self, targets, batch_size, scale_tensor):\n",
    "        \"\"\"Preprocess targets by converting to tensor format and scaling coordinates.\"\"\"\n",
    "        nl, ne = targets.shape\n",
    "        if nl == 0:\n",
    "            out = torch.zeros(batch_size, 0, ne - 1, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]  # image index\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            counts = counts.to(dtype=torch.int32)\n",
    "            out = torch.zeros(batch_size, counts.max(), ne - 1, device=self.device)\n",
    "            for j in range(batch_size):\n",
    "                matches = i == j\n",
    "                if n := matches.sum():\n",
    "                    out[j, :n] = targets[matches, 1:]\n",
    "            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\n",
    "        return out\n",
    "\n",
    "    def bbox_decode(self, anchor_points, pred_dist):\n",
    "        \"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\n",
    "        if self.use_dfl:\n",
    "            b, a, c = pred_dist.shape  # batch, anchors, channels\n",
    "            pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "            # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "            # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\n",
    "        return dist2bbox(pred_dist, anchor_points, xywh=False)\n",
    "\n",
    "    def __call__(self, preds, batch):\n",
    "        \"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\"\"\"\n",
    "        ultralytics.utils.LOGGER.info(\"pickles!\")\n",
    "        loss = torch.zeros(3, device=self.device)  # box, cls, dfl\n",
    "        feats = preds[1] if isinstance(preds, tuple) else preds\n",
    "        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n",
    "            (self.reg_max * 4, self.nc), 1\n",
    "        )\n",
    "\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dtype = pred_scores.dtype\n",
    "        batch_size = pred_scores.shape[0]\n",
    "        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n",
    "        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n",
    "\n",
    "        # Targets\n",
    "        targets = torch.cat((batch[\"batch_idx\"].view(-1, 1), batch[\"cls\"].view(-1, 1), batch[\"bboxes\"]), 1)\n",
    "        targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\n",
    "        gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0.0)\n",
    "\n",
    "        # Pboxes\n",
    "        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n",
    "        # dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\n",
    "        # dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\n",
    "\n",
    "        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n",
    "            # pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\n",
    "            pred_scores.detach().sigmoid(),\n",
    "            (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n",
    "            anchor_points * stride_tensor,\n",
    "            gt_labels,\n",
    "            gt_bboxes,\n",
    "            mask_gt,\n",
    "        )\n",
    "\n",
    "        target_scores_sum = max(target_scores.sum(), 1)\n",
    "\n",
    "        # Cls loss\n",
    "        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\n",
    "        loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n",
    "\n",
    "        # Bbox loss\n",
    "        if fg_mask.sum():\n",
    "            target_bboxes /= stride_tensor\n",
    "            loss[0], loss[2] = self.bbox_loss(\n",
    "                pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask\n",
    "            )\n",
    "\n",
    "        loss[0] *= self.hyp.box  # box gain\n",
    "        loss[1] *= self.hyp.cls  # cls gain\n",
    "        loss[2] *= self.hyp.dfl  # dfl gain\n",
    "\n",
    "        return loss * batch_size, loss.detach()  # loss(box, cls, dfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6455e-3e9a-4ffc-a7c7-12bc0a41ddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964742e2-49f6-4e78-b187-18171544c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultralytics.utils.loss.v8DetectionLoss = v8DetectionHierarchicalLoss\n",
    "sys.modules['ultralytics.utils.loss'].v8DetectionLoss = v8DetectionHierarchicalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523216e8-c857-48f6-ab91-8d2d15a8cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '../data'\n",
    "UPLOAD_URL = 'https://storage.googleapis.com/nmfs_odp_hq/nodd_tools/datasets/oceaneyes/annotation_number_balanced_sample/annotations.json'\n",
    "DOWNLOAD_PATH = os.path.join(DATA, 'download')\n",
    "COCO_PATH = os.path.join(DOWNLOAD_PATH, 'annotations.json')\n",
    "YOLO_PATH = os.path.join(DOWNLOAD_PATH, 'yolo_training_data')\n",
    "IMAGES_PATH = os.path.join(YOLO_PATH, 'annotations', 'images')\n",
    "os.makedirs(YOLO_PATH, exist_ok=True)\n",
    "os.makedirs(DOWNLOAD_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2205cbc8-9015-48ab-b834-f10a32e2ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "uploaded_coco_file = requests.get(UPLOAD_URL)\n",
    "with open(COCO_PATH, 'wb') as f:\n",
    "    f.write(uploaded_coco_file.content)\n",
    "    coco = pycocotools.coco.COCO(COCO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5c50cb-5844-4ee4-aed3-2731deb2ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: 2024\n",
      "version: 0.1\n",
      "description: https://www.zooniverse.org/projects/benjamin-dot-richards/oceaneyes/about/research\n",
      "contributor: None\n",
      "url: None\n",
      "date_created: 2025-02-06T20:56:54.886937+00:00\n"
     ]
    }
   ],
   "source": [
    "coco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6358262a-4a16-494a-8836-17e2b6705056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotations /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/annotations.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 1582.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO data converted successfully.\n",
      "Results saved to /home/guest/Projects/NOAA/research/hierarchical_yolo/notebooks/coco_converted\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "downloaded 0/200 images (t=0.0s)\n",
      "downloaded 1/200 images (t=0.0s)\n",
      "downloaded 2/200 images (t=0.0s)\n",
      "downloaded 3/200 images (t=0.0s)\n",
      "downloaded 4/200 images (t=0.0s)\n",
      "downloaded 5/200 images (t=0.0s)\n",
      "downloaded 6/200 images (t=0.0s)\n",
      "downloaded 7/200 images (t=0.0s)\n",
      "downloaded 8/200 images (t=0.0s)\n",
      "downloaded 9/200 images (t=0.0s)\n",
      "downloaded 10/200 images (t=0.0s)\n",
      "downloaded 11/200 images (t=0.0s)\n",
      "downloaded 12/200 images (t=0.0s)\n",
      "downloaded 13/200 images (t=0.0s)\n",
      "downloaded 14/200 images (t=0.0s)\n",
      "downloaded 15/200 images (t=0.0s)\n",
      "downloaded 16/200 images (t=0.0s)\n",
      "downloaded 17/200 images (t=0.0s)\n",
      "downloaded 18/200 images (t=0.0s)\n",
      "downloaded 19/200 images (t=0.0s)\n",
      "downloaded 20/200 images (t=0.0s)\n",
      "downloaded 21/200 images (t=0.0s)\n",
      "downloaded 22/200 images (t=0.0s)\n",
      "downloaded 23/200 images (t=0.0s)\n",
      "downloaded 24/200 images (t=0.0s)\n",
      "downloaded 25/200 images (t=0.0s)\n",
      "downloaded 26/200 images (t=0.0s)\n",
      "downloaded 27/200 images (t=0.0s)\n",
      "downloaded 28/200 images (t=0.0s)\n",
      "downloaded 29/200 images (t=0.0s)\n",
      "downloaded 30/200 images (t=0.0s)\n",
      "downloaded 31/200 images (t=0.0s)\n",
      "downloaded 32/200 images (t=0.0s)\n",
      "downloaded 33/200 images (t=0.0s)\n",
      "downloaded 34/200 images (t=0.0s)\n",
      "downloaded 35/200 images (t=0.0s)\n",
      "downloaded 36/200 images (t=0.0s)\n",
      "downloaded 37/200 images (t=0.0s)\n",
      "downloaded 38/200 images (t=0.0s)\n",
      "downloaded 39/200 images (t=0.0s)\n",
      "downloaded 40/200 images (t=0.0s)\n",
      "downloaded 41/200 images (t=0.0s)\n",
      "downloaded 42/200 images (t=0.0s)\n",
      "downloaded 43/200 images (t=0.0s)\n",
      "downloaded 44/200 images (t=0.0s)\n",
      "downloaded 45/200 images (t=0.0s)\n",
      "downloaded 46/200 images (t=0.0s)\n",
      "downloaded 47/200 images (t=0.0s)\n",
      "downloaded 48/200 images (t=0.0s)\n",
      "downloaded 49/200 images (t=0.0s)\n",
      "downloaded 50/200 images (t=0.0s)\n",
      "downloaded 51/200 images (t=0.0s)\n",
      "downloaded 52/200 images (t=0.0s)\n",
      "downloaded 53/200 images (t=0.0s)\n",
      "downloaded 54/200 images (t=0.0s)\n",
      "downloaded 55/200 images (t=0.0s)\n",
      "downloaded 56/200 images (t=0.0s)\n",
      "downloaded 57/200 images (t=0.0s)\n",
      "downloaded 58/200 images (t=0.0s)\n",
      "downloaded 59/200 images (t=0.0s)\n",
      "downloaded 60/200 images (t=0.0s)\n",
      "downloaded 61/200 images (t=0.0s)\n",
      "downloaded 62/200 images (t=0.0s)\n",
      "downloaded 63/200 images (t=0.0s)\n",
      "downloaded 64/200 images (t=0.0s)\n",
      "downloaded 65/200 images (t=0.0s)\n",
      "downloaded 66/200 images (t=0.0s)\n",
      "downloaded 67/200 images (t=0.0s)\n",
      "downloaded 68/200 images (t=0.0s)\n",
      "downloaded 69/200 images (t=0.0s)\n",
      "downloaded 70/200 images (t=0.0s)\n",
      "downloaded 71/200 images (t=0.0s)\n",
      "downloaded 72/200 images (t=0.0s)\n",
      "downloaded 73/200 images (t=0.0s)\n",
      "downloaded 74/200 images (t=0.0s)\n",
      "downloaded 75/200 images (t=0.0s)\n",
      "downloaded 76/200 images (t=0.0s)\n",
      "downloaded 77/200 images (t=0.0s)\n",
      "downloaded 78/200 images (t=0.0s)\n",
      "downloaded 79/200 images (t=0.0s)\n",
      "downloaded 80/200 images (t=0.0s)\n",
      "downloaded 81/200 images (t=0.0s)\n",
      "downloaded 82/200 images (t=0.0s)\n",
      "downloaded 83/200 images (t=0.0s)\n",
      "downloaded 84/200 images (t=0.0s)\n",
      "downloaded 85/200 images (t=0.0s)\n",
      "downloaded 86/200 images (t=0.0s)\n",
      "downloaded 87/200 images (t=0.0s)\n",
      "downloaded 88/200 images (t=0.0s)\n",
      "downloaded 89/200 images (t=0.0s)\n",
      "downloaded 90/200 images (t=0.0s)\n",
      "downloaded 91/200 images (t=0.0s)\n",
      "downloaded 92/200 images (t=0.0s)\n",
      "downloaded 93/200 images (t=0.0s)\n",
      "downloaded 94/200 images (t=0.0s)\n",
      "downloaded 95/200 images (t=0.0s)\n",
      "downloaded 96/200 images (t=0.0s)\n",
      "downloaded 97/200 images (t=0.0s)\n",
      "downloaded 98/200 images (t=0.0s)\n",
      "downloaded 99/200 images (t=0.0s)\n",
      "downloaded 100/200 images (t=0.0s)\n",
      "downloaded 101/200 images (t=0.0s)\n",
      "downloaded 102/200 images (t=0.0s)\n",
      "downloaded 103/200 images (t=0.0s)\n",
      "downloaded 104/200 images (t=0.0s)\n",
      "downloaded 105/200 images (t=0.0s)\n",
      "downloaded 106/200 images (t=0.0s)\n",
      "downloaded 107/200 images (t=0.0s)\n",
      "downloaded 108/200 images (t=0.0s)\n",
      "downloaded 109/200 images (t=0.0s)\n",
      "downloaded 110/200 images (t=0.0s)\n",
      "downloaded 111/200 images (t=0.0s)\n",
      "downloaded 112/200 images (t=0.0s)\n",
      "downloaded 113/200 images (t=0.0s)\n",
      "downloaded 114/200 images (t=0.0s)\n",
      "downloaded 115/200 images (t=0.0s)\n",
      "downloaded 116/200 images (t=0.0s)\n",
      "downloaded 117/200 images (t=0.0s)\n",
      "downloaded 118/200 images (t=0.0s)\n",
      "downloaded 119/200 images (t=0.0s)\n",
      "downloaded 120/200 images (t=0.0s)\n",
      "downloaded 121/200 images (t=0.0s)\n",
      "downloaded 122/200 images (t=0.0s)\n",
      "downloaded 123/200 images (t=0.0s)\n",
      "downloaded 124/200 images (t=0.0s)\n",
      "downloaded 125/200 images (t=0.0s)\n",
      "downloaded 126/200 images (t=0.0s)\n",
      "downloaded 127/200 images (t=0.0s)\n",
      "downloaded 128/200 images (t=0.0s)\n",
      "downloaded 129/200 images (t=0.0s)\n",
      "downloaded 130/200 images (t=0.0s)\n",
      "downloaded 131/200 images (t=0.0s)\n",
      "downloaded 132/200 images (t=0.0s)\n",
      "downloaded 133/200 images (t=0.0s)\n",
      "downloaded 134/200 images (t=0.0s)\n",
      "downloaded 135/200 images (t=0.0s)\n",
      "downloaded 136/200 images (t=0.0s)\n",
      "downloaded 137/200 images (t=0.0s)\n",
      "downloaded 138/200 images (t=0.0s)\n",
      "downloaded 139/200 images (t=0.0s)\n",
      "downloaded 140/200 images (t=0.0s)\n",
      "downloaded 141/200 images (t=0.0s)\n",
      "downloaded 142/200 images (t=0.0s)\n",
      "downloaded 143/200 images (t=0.0s)\n",
      "downloaded 144/200 images (t=0.0s)\n",
      "downloaded 145/200 images (t=0.0s)\n",
      "downloaded 146/200 images (t=0.0s)\n",
      "downloaded 147/200 images (t=0.0s)\n",
      "downloaded 148/200 images (t=0.0s)\n",
      "downloaded 149/200 images (t=0.0s)\n",
      "downloaded 150/200 images (t=0.0s)\n",
      "downloaded 151/200 images (t=0.0s)\n",
      "downloaded 152/200 images (t=0.0s)\n",
      "downloaded 153/200 images (t=0.0s)\n",
      "downloaded 154/200 images (t=0.0s)\n",
      "downloaded 155/200 images (t=0.0s)\n",
      "downloaded 156/200 images (t=0.0s)\n",
      "downloaded 157/200 images (t=0.0s)\n",
      "downloaded 158/200 images (t=0.0s)\n",
      "downloaded 159/200 images (t=0.0s)\n",
      "downloaded 160/200 images (t=0.0s)\n",
      "downloaded 161/200 images (t=0.0s)\n",
      "downloaded 162/200 images (t=0.0s)\n",
      "downloaded 163/200 images (t=0.0s)\n",
      "downloaded 164/200 images (t=0.0s)\n",
      "downloaded 165/200 images (t=0.0s)\n",
      "downloaded 166/200 images (t=0.0s)\n",
      "downloaded 167/200 images (t=0.0s)\n",
      "downloaded 168/200 images (t=0.0s)\n",
      "downloaded 169/200 images (t=0.0s)\n",
      "downloaded 170/200 images (t=0.0s)\n",
      "downloaded 171/200 images (t=0.0s)\n",
      "downloaded 172/200 images (t=0.0s)\n",
      "downloaded 173/200 images (t=0.0s)\n",
      "downloaded 174/200 images (t=0.0s)\n",
      "downloaded 175/200 images (t=0.0s)\n",
      "downloaded 176/200 images (t=0.0s)\n",
      "downloaded 177/200 images (t=0.0s)\n",
      "downloaded 178/200 images (t=0.0s)\n",
      "downloaded 179/200 images (t=0.0s)\n",
      "downloaded 180/200 images (t=0.0s)\n",
      "downloaded 181/200 images (t=0.0s)\n",
      "downloaded 182/200 images (t=0.0s)\n",
      "downloaded 183/200 images (t=0.0s)\n",
      "downloaded 184/200 images (t=0.0s)\n",
      "downloaded 185/200 images (t=0.0s)\n",
      "downloaded 186/200 images (t=0.0s)\n",
      "downloaded 187/200 images (t=0.0s)\n",
      "downloaded 188/200 images (t=0.0s)\n",
      "downloaded 189/200 images (t=0.0s)\n",
      "downloaded 190/200 images (t=0.0s)\n",
      "downloaded 191/200 images (t=0.0s)\n",
      "downloaded 192/200 images (t=0.0s)\n",
      "downloaded 193/200 images (t=0.0s)\n",
      "downloaded 194/200 images (t=0.0s)\n",
      "downloaded 195/200 images (t=0.0s)\n",
      "downloaded 196/200 images (t=0.0s)\n",
      "downloaded 197/200 images (t=0.0s)\n",
      "downloaded 198/200 images (t=0.0s)\n",
      "downloaded 199/200 images (t=0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pycocowriter.coco2yolo.coco2yolo(DOWNLOAD_PATH, YOLO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db4e6a-4077-4fc6-ab82-5f9999738574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05e415ec-f0c2-48e2-8633-867199e3f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: the yolov8.yaml model file downloaded from Ultralytics needs manual editing for the number of classes\n",
    "YOLO_YAML = os.path.join(DATA, 'yolov8.yaml')\n",
    "YOLO_BASE_MODEL = os.path.join(DATA, 'yolov8n.pt')\n",
    "YOLO_TRAIN_YAML = os.path.join(YOLO_PATH, 'train.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3438182-1a11-466e-8fac-c30c276de0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d1985-c845-4018-8328-daf8502727fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac7f6069-bf43-40e1-ab11-450c5425e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è no model scale passed. Assuming scale='n'.\n",
      "Transferred 319/355 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(YOLO_YAML).load(YOLO_BASE_MODEL)  # build a new model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfbb5b7a-60f5-43aa-bd2b-c463e665843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.criterion = v8DetectionHierarchicalLoss(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd0538d-6b0c-486c-91a2-52b63ad8f747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.v8DetectionHierarchicalLoss at 0x70ed74c11340>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3c7e87-6e3f-466c-b4fe-4065202b6970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.101 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.100 üöÄ Python-3.12.3 torch-2.6.0+cu124 CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=../data/yolov8.yaml, data=../data/download/yolo_training_data/train.yaml, epochs=5, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=[], workers=8, project=None, name=train19, exist_ok=False, pretrained=../data/yolov8n.pt, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/guest/Projects/NOAA/research/hierarchical_yolo/runs/detect/train19\n",
      "WARNING ‚ö†Ô∏è no model scale passed. Assuming scale='n'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
      "YOLOv8 summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/yolo_training_data/annotations/labels.cache... 200 images, 0 backgrounds, 1 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/2\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/yolo_training_data/annotations/images/20190913_201709_20190913.203153.002.010131.jpg: ignoring corrupt image/label: negative label values [-0.00068681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/yolo_training_data/annotations/labels.cache... 200 images, 0 backgrounds, 1 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/yolo_training_data/annotations/images/20190913_201709_20190913.203153.002.010131.jpg: ignoring corrupt image/label: negative label values [-0.00068681]\n",
      "Plotting labels to /home/guest/Projects/NOAA/research/hierarchical_yolo/runs/detect/train19/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(26745dc0838a4a0ca9863aa58c736132) to /home/guest/Projects/NOAA/research/hierarchical_yolo/runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri /home/guest/Projects/NOAA/research/hierarchical_yolo/runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mWARNING ‚ö†Ô∏è Failed to initialize: Changing param values is not allowed. Param with key='name' was already logged with value='train18' for run ID='26745dc0838a4a0ca9863aa58c736132'. Attempted logging new value 'train19'.\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mWARNING ‚ö†Ô∏è Not tracking this run\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/home/guest/Projects/NOAA/research/hierarchical_yolo/runs/detect/train19\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      2.601      4.278      1.574        105        640:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [01:06<00:04,  1.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mYOLO_TRAIN_YAML\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:791\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    790\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:211\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:385\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m.amp):\n\u001b[32m    384\u001b[39m     batch = \u001b[38;5;28mself\u001b[39m.preprocess_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m.loss = loss.sum()\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:119\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[32m    107\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m \u001b[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:299\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.criterion = \u001b[38;5;28mself\u001b[39m.init_criterion()\n\u001b[32m    298\u001b[39m preds = \u001b[38;5;28mself\u001b[39m.forward(batch[\u001b[33m\"\u001b[39m\u001b[33mimg\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/utils/loss.py:228\u001b[39m, in \u001b[36mv8DetectionLoss.__call__\u001b[39m\u001b[34m(self, preds, batch)\u001b[39m\n\u001b[32m    224\u001b[39m pred_bboxes = \u001b[38;5;28mself\u001b[39m.bbox_decode(anchor_points, pred_distri)  \u001b[38;5;66;03m# xyxy, (b, h*w, 4)\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m _, target_bboxes, target_scores, fg_mask, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massigner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\u001b[39;49;00m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bboxes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m target_scores_sum = \u001b[38;5;28mmax\u001b[39m(target_scores.sum(), \u001b[32m1\u001b[39m)\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Cls loss\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/utils/tal.py:77\u001b[39m, in \u001b[36mTaskAlignedAssigner.forward\u001b[39m\u001b[34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     69\u001b[39m         torch.full_like(pd_scores[..., \u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.bg_idx),\n\u001b[32m     70\u001b[39m         torch.zeros_like(pd_bboxes),\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m         torch.zeros_like(pd_scores[..., \u001b[32m0\u001b[39m]),\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m torch.cuda.OutOfMemoryError:\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# Move tensors to CPU, compute, then move back to original device\u001b[39;00m\n\u001b[32m     80\u001b[39m     LOGGER.warning(\u001b[33m\"\u001b[39m\u001b[33mWARNING: CUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/utils/tal.py:104\u001b[39m, in \u001b[36mTaskAlignedAssigner._forward\u001b[39m\u001b[34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n\u001b[32m     86\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m    Compute the task-aligned assignment.\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m \u001b[33;03m        target_gt_idx (torch.Tensor): Target ground truth indices with shape (bs, num_total_anchors).\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     mask_pos, align_metric, overlaps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_pos_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     target_gt_idx, fg_mask, mask_pos = \u001b[38;5;28mself\u001b[39m.select_highest_overlaps(mask_pos, overlaps, \u001b[38;5;28mself\u001b[39m.n_max_boxes)\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# Assigned target\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/utils/tal.py:139\u001b[39m, in \u001b[36mTaskAlignedAssigner.get_pos_mask\u001b[39m\u001b[34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_pos_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n\u001b[32m    123\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m    Get positive mask for each ground truth box.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03m        overlaps (torch.Tensor): Overlaps between predicted and ground truth boxes with shape (bs, max_num_obj, h*w).\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     mask_in_gts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselect_candidates_in_gts\u001b[49m\u001b[43m(\u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Get anchor_align metric, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[32m    141\u001b[39m     align_metric, overlaps = \u001b[38;5;28mself\u001b[39m.get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts * mask_gt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/utils/tal.py:295\u001b[39m, in \u001b[36mTaskAlignedAssigner.select_candidates_in_gts\u001b[39m\u001b[34m(xy_centers, gt_bboxes, eps)\u001b[39m\n\u001b[32m    293\u001b[39m bs, n_boxes, _ = gt_bboxes.shape\n\u001b[32m    294\u001b[39m lt, rb = gt_bboxes.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m).chunk(\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# left-top, right-bottom\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m bbox_deltas = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy_centers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrb\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mxy_centers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m.view(bs, n_boxes, n_anchors, -\u001b[32m1\u001b[39m)\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bbox_deltas.amin(\u001b[32m3\u001b[39m).gt_(eps)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = model.train(\n",
    "    data=YOLO_TRAIN_YAML, \n",
    "    epochs=5, imgsz=640, device=[], batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98783e2-d11e-41d1-83c2-a10a12bcf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_img = os.path.join(IMAGES_PATH, str(np.random.choice(os.listdir(IMAGES_PATH))))\n",
    "random_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ac518-367d-45a0-928c-84f4f63c59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LOCATION = os.path.join('..','runs','detect')\n",
    "trained_models = os.listdir(os.path.join('..','runs','detect'))\n",
    "model_numbers = map(lambda x: int(x[len('train'):]) if len(x) > len('train') else 0, trained_models)\n",
    "latest_model = 'train' + str(max(model_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a45d2a-c07d-4810-bf73-01d7bc0c1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = YOLO(\n",
    "    YOLO_YAML\n",
    ").load(os.path.join(MODEL_LOCATION, latest_model, 'weights', 'best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e0c0a-ff41-4da1-b488-1a8e4aa49dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trained_model.predict(random_img, verbose=False, device=[], stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466de62-7ec7-47e5-bad6-46d6f8726517",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = next(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1e1b7-ad91-4447-8e90-3ccab5291edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070c873-1928-4c41-be6d-c943a40364af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79949504-3e7a-404c-a85b-2b4ba982e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.save('pickles.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba6f2e-2649-4071-9d98-52278698423d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
